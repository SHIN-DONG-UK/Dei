from langchain_ollama import OllamaLLM # LLM ëª¨ë¸
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.vectorstores import Chroma
from langchain.embeddings.openai import OpenAIEmbeddings
import os
from dotenv import load_dotenv

class C104_RAG():
    def __init__(self):
        # .env íŒŒì¼ ë¡œë“œ
        load_dotenv()
        # 1. OpenAI API í‚¤ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
        openai_api_key = os.getenv("OPENAI_API_KEY")
        if not openai_api_key:
<<<<<<< HEAD
<<<<<<< HEAD
            raise ValueError("ğŸ”´ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        # ollamaLLM
        self.llm = OllamaLLM(model="exaone-2.4b-4bit")
        # ChromaDB ì»¬ë ‰ì…˜ ì´ˆê¸°í™”
        self.CHROMA_DB_PATH = "/home/god/integration_ws/src/llm_pkg/500_manuals_db"
=======
            raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        # ollamaLLM
        self.llm = OllamaLLM(model="exaone-2.4b-4bit")
        # ChromaDB ì»¬ë ‰ì…˜ ì´ˆê¸°í™”
        self.CHROMA_DB_PATH = "/home/c104/S12P11C104/ros/src/AI/llm_pkg/rag_db"
>>>>>>> ros2
=======
            raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        # ollamaLLM
        self.llm = OllamaLLM(model="exaone-2.4b-4bit")
        # ChromaDB ì»¬ë ‰ì…˜ ì´ˆê¸°í™”
        self.CHROMA_DB_PATH = "/home/c104/S12P11C104/ros/src/AI/llm_pkg/rag_db"
>>>>>>> 961d28f (AI : vector store update)
        self.embedding_model = OpenAIEmbeddings(model="text-embedding-ada-002")
        self.vector_db = Chroma(persist_directory=self.CHROMA_DB_PATH, embedding_function=self.embedding_model)
        print("RAG ì„œë¹„ìŠ¤ ì¤€ë¹„ ì™„ë£Œ!")
    
    def getResult(self, text):
        # LLM ìˆ˜í–‰
        return self.llm.invoke(text)
    
    def generate_rag_response(self, prompt, query, top_k=5):
        """ì‚¬ìš©ì ì§ˆì˜(query)ì— ëŒ€í•œ RAG ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
        # 1. ChromaDBì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        results = self.vector_db.max_marginal_relevance_search(
            query, 
            k=5,       # ìµœì¢…ì ìœ¼ë¡œ LLMì—ê²Œ ì œê³µí•  ë¬¸ì„œ 5ê°œ ê°€ì ¸ì˜¤ê¸°
            fetch_k=10, # ìœ ì‚¬ë„ ë†’ì€ ìƒìœ„ ë¬¸ì„œ 10ê°œ ê°€ì ¸ì˜¤ê¸°
            # filter={"category": "air_conditioner"}  # ì—ì–´ì»¨ ê´€ë ¨ ë¬¸ì„œë§Œ ê²€ìƒ‰
        )
        # search_results = self.vector_db.similarity_search(query, k=top_k)

        # 2. ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        retrieved_context = "\n\n".join([doc.page_content for doc in results])

        # 3. ì‚¬ìš©ì ì •ì˜ promptë¥¼ í™œìš©í•˜ì—¬ ìµœì¢… í”„ë¡¬í”„íŠ¸ ìƒì„±
        final_prompt = prompt.format(query=query, context=retrieved_context)

        # 4. OpenAI LLMì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±
        response = self.llm.invoke(final_prompt)
        return response
    
    def rank_documents(self, documents, query):
        """LLMì„ í™œìš©í•˜ì—¬ ë¬¸ì„œì˜ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ê³  ìƒìœ„ 5ê°œ ë¬¸ì„œ ì„ íƒ"""
        def get_score(doc):
            score_str = self.llm.predict(
                f"ì§ˆë¬¸: {query}\nì•„ë˜ ë¬¸ì„œì™€ ì§ˆë¬¸ì˜ ê´€ë ¨ì„±ì„ 1ë¶€í„° 10ê¹Œì§€ì˜ ìˆ«ìë¡œ í‰ê°€í•´ì¤˜. ì˜¤ì§ ìˆ«ìë§Œ ì¶œë ¥í•´ì¤˜:\n{doc.page_content}"
            ).strip()
            try:
                return float(score_str)
            except ValueError:
                return 0.0  # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì ìˆ˜ 0 ë¶€ì—¬

        ranked_docs = sorted(documents, key=get_score, reverse=True)
        return ranked_docs[:5]
    
    def generate_rag_response(self, prompt, query, top_k=5):
        """ì‚¬ìš©ìì˜ ì§ˆì˜(query)ì— ëŒ€í•´ RAG ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
        # 1. mmr ê²€ìƒ‰
        search_results = self.vector_db.max_marginal_relevance_search(query, k=top_k, fetch_k=10)

        # 2. ë¬¸ì„œ ì¬í‰ê°€ í›„ ìƒìœ„ ë¬¸ì„œ ì„ íƒ
        ranked_results = self.rank_documents(search_results, query)
        retrieved_context = "\n\n".join([doc.page_content for doc in ranked_results])

        # 3. ì‚¬ìš©ì ì •ì˜ promptë¥¼ í™œìš©í•˜ì—¬ ìµœì¢… í”„ë¡¬í”„íŠ¸ ìƒì„±
        final_prompt = prompt.format(query=query, context=retrieved_context)

        # 4. ë‹µë³€ ìƒì„±
        answer = self.llm.invoke(final_prompt)

        return answer
    
    def stop(self):
        self.llm.stop()